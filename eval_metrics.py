
"""
This script allows for the standardized evaluation metrics
such as F1, BLEU-4, ROUGE-L, METEOR, etc. to be computed
against a set of reference responses.
"""
import argparse
import glob
import logging
import os
import pprint
from collections import Counter

import nlgeval
import nlp
import nltk
from nltk import meteor


class ReferenceMetric(object):
    """
    Metric that requires a reference sentence for each
    hypothesis to compute
    """
    def compute(self, hypotheses, references):
        raise NotImplementedError("Implement the compute method!")


class ReferenceFreeMetric(object):
    """
    Metric that does not require a reference sentence
    """

    def compute(self, hypotheses):
        raise NotImplementedError("Implement the compute method!")


class NLPReferenceMetric(ReferenceMetric):
    """
    Reference dependent metrics that are part of the
    Huggingface NLP library
    """
    def __init__(self, module, compute_args={}):
        self.scorer = nlp.load_metric(module)
        self.compute_args = compute_args

    def compute(self, hypotheses, references):
        return self.scorer.compute(hypotheses, references, **self.compute_args)


class BLEUMetric(NLPReferenceMetric):
    def __init__(self):
        super().__init__('bleu')

    def __repr__(self):
        return 'BLEU-4'

    def compute(self, hypotheses, references):
        return super().compute([hyp.split() for hyp in hypotheses], [[ref.split()] for ref in references])


class RougeMetric(NLPReferenceMetric):
    def __init__(self):
        super().__init__('rouge')

    def __repr__(self):
        return 'ROUGE'

class BertScoreMetric(NLPReferenceMetric):
    def __init__(self):
        self.arg_dict = {"lang": "en"}
        super().__init__('bertscore', self.arg_dict)

    def __repr__(self):
        return f'BertScore({self.arg_dict})'

    def compute(self, hypotheses, references):
        return sum(super().compute(hypotheses, references)['f1']) / len(hypotheses)


class UnigramFScoreMetric(ReferenceMetric):
    def __init__(self, beta=1):
        self.beta = beta
        self.beta_squared = beta * beta  # Fbeta uses (beta^2 as weighting factor)

    def _f_beta(self, pred, true):
        common = Counter(true) & Counter(pred)
        num_same = sum(common.values())

        if num_same == 0:
            return 0

        prec = num_same / len(pred)
        rec = num_same / len(true)

        f_beta = ((self.beta_squared + 1) * prec * rec) / (self.beta_squared * prec + rec)
        return f_beta

    def compute(self, hypotheses, references):
        return sum([self._f_beta(hyp.split(), ref.split()) for hyp, ref in zip(hypotheses, references)]) / len(references)

    def __repr__(self):
        return f'F{self.beta}-score'

class MeteorMetric(ReferenceMetric):
    """
    Computes the average METEOR score across all examples
    """

    def compute(self, hypotheses, references):

        try:
            nltk.data.find('wordnet')
        except LookupError:
            nltk.download('wordnet')

        return sum([meteor([ref], hyp) for (ref, hyp) in zip(hypotheses, references)]) / len(references)

    def __repr__(self):
        return f'METEOR score'

class NGramDiversity(ReferenceFreeMetric):
    def __init__(self, n=1):
        self.n = n

    def _diversity(self, pred):
        n_grams = []

        for i in range(len(pred) - self.n + 1):
            n_grams.append(' '.join(pred[i:i + self.n]))

        if len(n_grams) == 0:
            return 0

        return len(set(n_grams)) / len(n_grams)

    def compute(self, hypotheses):
        return sum([self._diversity(hyp.split()) for hyp in hypotheses]) / len(hypotheses)

    def __repr__(self):
        return f'Utterance {self.n}-gram diversity'

class CorpusNGramDiversity(ReferenceFreeMetric):
    """
    Computes the number of unique n-grams that were generated by the model
    out of the total number of n-grams present
    """
    def __init__(self, n=1):
        self.n = n

    def compute(self, hypotheses):
        n_grams = []

        for hyp in hypotheses:
            pred = hyp.split()
            for i in range(len(pred) - self.n + 1):
                n_grams.append(' '.join(pred[i:i + self.n]))

        if len(n_grams) == 0:
            return 0

        return len(set(n_grams)) / len(n_grams)

    def __repr__(self):
        return f'Corpus {self.n}-gram diversity'

class NLGEval(ReferenceMetric):
    """
    Runs the full NLGEval pipeline which computes multiple machine translation
    metrics:
    1. BLEU
    2. METEOR
    3. ROUGE
    3. CIDEr
    4. Skip Thought
    5. Embedding Average
    6. Vector Extrema
    7. Greedy matching
    """

    def __init__(self, hypothesis_file, reference_file):
        self.metrics_dict = nlgeval.compute_metrics(hypothesis=hypothesis_file,
                                                    references=[reference_file], no_skipthoughts=True, no_glove=True)
    def __repr__(self):
        return 'NLGEval metrics'

    def compute(self, hypotheses, references):
        return pprint.pformat(self.metrics_dict)

class USRMetric(ReferenceFreeMetric):
    """
    Computes the USR score as defined by Mehri and Eskenazi (2019)

    Currently the code computes the following:
    1. MLM objective
    """
    def _make_scoring_file(self, context_file, hypothesis_file):

        scratch_path = 'submissions/scratch'
        with open(context_file, 'r') as context_, open(hypothesis_file, 'r') as hypothesis_, open(scratch_path, 'w') as scratch_file:
            context_lines = [line.strip() for line in context_]
            hypothesis_lines = [line.strip() for line in hypothesis_]

            for (c, h) in zip(context_lines, hypothesis_lines):
                scratch_file.writelines(f'{c} _eos _go {h}\n')

        return scratch_path

    def __init__(self, context_file, hypothesis_file):
        scores = self.compute_mlm_scores(context_file, hypothesis_file)

        self.results = scores

    def compute_mlm_scores(self, context_file, hypothesis_file):
        scoring_file = self._make_scoring_file(context_file, hypothesis_file)
        args = self.build_args(hypothesis_file, scoring_file)
        from usr.examples.run_lm_finetuning import evaluate, MODEL_CLASSES, WEIGHTS_NAME
        config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]
        config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,
                                              cache_dir=args.cache_dir if args.cache_dir else None)
        tokenizer = tokenizer_class.from_pretrained(
            args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,
            do_lower_case=args.do_lower_case,
            cache_dir=args.cache_dir if args.cache_dir else None)
        if args.block_size <= 0:
            args.block_size = tokenizer.max_len_single_sentence
        model = model_class.from_pretrained(args.model_name_or_path,
                                            from_tf=bool('.ckpt' in args.model_name_or_path),
                                            config=config,
                                            cache_dir=args.cache_dir if args.cache_dir else None)
        model.to(args.device)
        result = None
        checkpoints = [args.output_dir]
        if args.eval_all_checkpoints:
            checkpoints = list(
                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True)))
            logging.getLogger("transformers.modeling_utils").setLevel(logging.WARN)  # Reduce logging
        for checkpoint in checkpoints:
            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else ""
            prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else ""

            model = model_class.from_pretrained(checkpoint)
            model.to(args.device)
            result = evaluate(args, model, tokenizer, prefix=prefix)

            # result = dict((k + '_{}'.format(global_step), v) for k, v in result.items())
            # results.update(result)
        return result

    def build_args(self, hypothesis_file, scoring_file):
        args = argparse.Namespace()
        args.output_dir = 'usr/examples/roberta_ft'
        args.model_type = 'roberta'
        args.train_data_file = hypothesis_file
        args.per_gpu_eval_batch_size = 1
        args.model_name_or_path = 'roberta-base'
        args.eval_data_file = scoring_file
        args.do_eval = True
        args.mlm = True
        args.device = "cpu"
        args.local_rank = -1
        args.n_gpu = 0
        args.config_name = ""
        args.cache_dir = ""
        args.tokenizer_name = ""
        args.do_lower_case = False
        args.eval_all_checkpoints = False
        args.block_size = -1
        return args

    def __repr__(self):
        return 'USR Metric Fine-tuned on Topical Chats'

    def compute(self, hypotheses):
        return self.results

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    # Currently support only single hypotheses scoring
    parser.add_argument('--context_file',
                        type=str,
                        default='processed_output/valid_freq.src')

    parser.add_argument('--predictions_file',
                        type=str,
                        default="submissions/submissions.txt",
                        help='File containing output predictions')
    parser.add_argument('--references_file',
                        type=str,
                        default='processed_output/valid_freq.tgt',
                        help='File containing the reference responses')
    args = parser.parse_args()

    with open(args.predictions_file, 'r') as predictions_file:
        predictions = [line
                           .strip()
                           .replace(".", " .").replace("?", " ?")
                           .replace(",", " ,")
                           .replace("'", " ' ")
                           .replace("dn't", "d n't")
                       for line in predictions_file]

    with open(args.references_file, 'r') as references_file:
        references = [line.replace("_go", "").replace("_eos", "").strip() for line in references_file]

    assert len(predictions) == len(references), "The number of predictions and references do not match!"

    for prediction in predictions:
        assert prediction != "", "Predictions cannot be empty!"

    metrics = [
        BLEUMetric(),
        RougeMetric(),
        BertScoreMetric(),
        MeteorMetric(),
        UnigramFScoreMetric(),
        NGramDiversity(n=1),
        NGramDiversity(n=2),
        CorpusNGramDiversity(n=1),
        CorpusNGramDiversity(n=2),
        NLGEval(args.predictions_file, args.references_file),
        USRMetric(args.context_file, args.predictions_file)
    ]

    print(f"Number of examples n={len(predictions)}\n")

    for metric in metrics:
        if isinstance(metric, ReferenceFreeMetric):
            print(metric, ":")
            print(metric.compute(predictions))
        else:
            print(metric, ":")
            print(metric.compute(predictions, references))
